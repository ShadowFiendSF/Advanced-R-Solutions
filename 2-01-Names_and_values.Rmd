```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = TRUE
)

Sys.setenv(LANGUAGE = "en")
library("methods")
```
# (PART) Foundations {-} 

# Names and values

```{r, include=FALSE}
library(lobstr)
library(magrittr)
```

## Binding basics

1. __<span style="color:red">Q</span>__: Explain the relationship between `a`, `b`, `c` and `d` in the following code:

    ```{r}
    a <- 1:10
    b <- a
    c <- b
    d <- 1:10
    ```
    
   __<span style="color:green">A</span>__: `a`, `b`, `c` point to the same object (with the same address in memory). This object has the value `1:10`. `d` points to a different object with the same value.

    ```{r}
    list_of_names <- list(a, b, c, d)
    obj_addrs(list_of_names)
    ```

<!-- ```{r} -->
<!-- # alternative code: -->
<!-- list(a = a, b = b, c = c, d = d) %>% -->
<!--   map_chr(obj_addr) -->
<!-- ``` -->

2. __<span style="color:red">Q</span>__: The following code accesses the mean function in multiple different ways. Do they all point to the same underlying function object? Verify with `lobstr::obj_addr()`.
    
    ```{r, eval = FALSE}
    mean
    base::mean
    get("mean")
    evalq(mean)
    match.fun("mean")
    ```
    
   __<span style="color:green">A</span>__: Yes, they point to the same object. We confirm this by looking at the address of the underlying function object.
       
    ```{r}
    mean_functions <- list(mean,
                           base::mean,
                           get("mean"),
                           evalq(mean),
                           match.fun("mean"))
    
    unique(obj_addrs(mean_functions))
    ```
    
3. __<span style="color:red">Q</span>__: By default, base R data import functions, like `read.csv()`, will automatically convert non-syntactic names to syntactic names. Why might this be problematic? What option allows you to suppress this behaviour?
    
   __<span style="color:green">A</span>__: When automatic and implicit (name) conversion occurrs, the prediction of a scripts output will be more difficult. For example when R is used non-interactively and some data is read, transformed and written, than the output may not contain the same names as the original data source. This could introduce problems in downstream analysis. To avoid automatic name conversion set `check.names=FALSE`.
    
4. __<span style="color:red">Q</span>__: What rules does `make.names()` use to convert non-syntactic names into syntactic names?
    
   __<span style="color:green">A</span>__: A valid name starts with a letter or a dot (which must not be followed by a number). It also consists of letters, numbers, dots and underscores only (`"_"` are allowed since R version 1.9.0).
   
   There are three main mechanisms to ensure syntactically valid names (see `?make.names`):
   - The variable name will be prepended by an `X` when names do not start with a letter or start with a dot followed by a number:
    
    ```{r}
    make.names("")
    make.names(".1")
    ```
    
   - (additionally) non-valid characters are replaced by a dot:
    
    ```{r}
    make.names("@")          # prepending + . replacement 
    make.names("  ")         # prepending + .. replacement
    make.names("non-valid")  # . replacement
    ```
    
   - reserved R keywords (see `?reserved`) are appended by a dot:
    
    ```{r}
    make.names("if")
    ```
    
   Interestingly, some of these transformations may also depend on the current locale (see `?make.names`):

   > The definition of a letter depends on the current locale, but only ASCII digits are considered to be digits.

5. __<span style="color:red">Q</span>__: I slightly simplified the rules that govern syntactic names. Why is `.123e1` not a syntactic name? Read `?make.names` for the full details.
    
   __<span style="color:green">A</span>__: `.123e1` is not a syntact name, because it starts with one dot which is followed by a number.

## Copy-on-modify

1. __<span style="color:red">Q</span>__: Why is `tracemem(1:10)` not useful?

   __<span style="color:green">A</span>__: Without a binding `1:10` will not stay in memory (there will be no reference) and it makes no sense to track an object for copies which doesn't exist. Also when we assign `1:10` to a name, it will be clear, that `1:10` will only be the value of the object created and there is no "general" object `1:10`, which one would wan't to track.

2. __<span style="color:red">Q</span>__: Explain why `tracemem()` shows two copies when you run this code. Hint: carefully look at the difference between this code and the code show earlier in the section.
     
    ```{r, results = FALSE}
    x <- c(1L, 2L, 3L)
    tracemem(x)
    
    x[[3]] <- 4
    ```
    
   __<span style="color:green">A</span>__: Initially `x` is an integer vector. In the replacement call a double is assigned to the third element of `x`. Because of R's coercion rules, a type conversion takes place, which affects the whole vector.

    ```{r, eval=FALSE}
    # two copies
    x <- 1:3
    tracemem(x)
    #> <0x66a4a70>
    
    x[[3]] <- 4
    #> tracemem[0x55eec7b3af38 -> 0x55eec774cc18]: 
    #> tracemem[0x55eec774cc18 -> 0x55eeca6ed5a8]: 
    ```
    
   By assigning an integer instead of a double one copy may be avoided:
    
    ```{r, eval=FALSE}
    # the same as 
    x <- 1:3
    tracemem(x)
    #> <0x55eec6940ae0>
    
    x[[3]] <- 4L
    #> tracemem[0x55eec7021e10 -> 0x55eecb99e788]: 
    x <- as.double(x)
    #> tracemem[0x55eecb99e788 -> 0x55eec93d9c18]:
    ```

   <!-- code-chunks above were hard-coded to fix knitr output. -->

3. __<span style="color:red">Q</span>__: Sketch out the relationship between the following objects:

    ```{r}
    a <- 1:10
    b <- list(a, a)
    c <- list(b, a, 1:10)
    ```
    
   __<span style="color:green">A</span>__: `a` contains a reference to an address with the value `1:10`. `b` contains a list of two references to the same address as `a`. `c` contains a list of `b` (containing two references to `a`), `a` (containing the same reference again) and a reference pointing to a different address containing the same value (`1:10`).

    ```{r, eval = FALSE}
    ref(c)
    #> █ [1:0x55eec93cbdd8] <list>    # c
    #> ├─█ [2:0x55eecb8246e8] <list>  # - b
    #> │ ├─[3:0x55eec7df4e98] <int>   # -- a
    #> │ └─[3:0x55eec7df4e98]         # -- a
    #> ├─[3:0x55eec7df4e98]           # - a
    #> └─[4:0x55eec7aa6968] <int>     # - 1:10
    ```


4. __<span style="color:red">Q</span>__: What happens when you run this code:

    ```{r}
    x <- list(1:10)
    x[[2]] <- x
    ```
    
   Draw a picture.

   __<span style="color:green">A</span>__: The initial reference tree of `x` shows, that the name `x` binds to a list object. This object contains a reference to the integer vector `1:10`.

    ```{r, eval=FALSE}
    x <- list(1:10)
    ref(x)
    #> █ [1:0x55853b74ff40] <list> 
    #> └─[2:0x534t3abffad8] <int> 
    ```

    ```{r, echo = FALSE}
    knitr::include_graphics("images/copy_on_modify_fig1.png")
    ```

   When `x` is assigned to an element of itself copy-on-modify takes place and the list is copied to a new address in memory.

    ```{r, eval=FALSE}
    tracemem(x)
    x[[2]] <- x
    #> tracemem[0x55853b74ff40 -> 0x5d553bacdcd8]:
    ```

   The list object previously bound to `x` is now referenced in the newly created list object. It is no longer bound to a name. The integer vector is referenced twice.

    ```{r, eval=FALSE}
    ref(x)
    #> █ [1:0x5d553bacdcd8] <list> 
    #> ├─[2:0x534t3abffad8] <int> 
    #> └─█ [3:0x55853b74ff40] <list> 
    #>   └─[2:0x534t3abffad8] 
    ```

    ```{r, echo = FALSE}
    knitr::include_graphics("images/copy_on_modify_fig2.png")
    ```

## Object size

1. __<span style="color:red">Q</span>__: In the following example, why are `object.size(y)` and `obj_size(y)` so radically different? Consult the documentation of `object.size()`.

    ```{r}
    y <- rep(list(runif(1e4)), 100)
    
    object.size(y)
    obj_size(y)
    ```
    
   __<span style="color:green">A</span>__: `object.size()` doesn't account for shared elements within lists. Therefore, the results differ by a factor of ~ 100.

2. __<span style="color:red">Q</span>__: Take the following list. Why is its size somewhat misleading?

    ```{r, return = FALSE}
    x <- list(mean, sd, var)
    # obj_size(x)
    #> 16,928 B
    ```
    __<span style="color:green">A</span>__: It is somewhat misleading, because all three functions are built-in to R as part of the base and stats packages and hence always loaded. 
    
    Via the following calculations we can see that this holds for about `r base_env_names <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "Autoloads", "package:base"); base_env_list <- sapply(base_env_names, function(x) mget(ls(x, all = TRUE), as.environment(x))); sum(lengths(base_env_list))` objects which are usually loaded by default and use about `r round(sum(sapply(base_env_list, lobstr::obj_size)) / 1024^2, 2)` MB:
  
  ```{r}
  base_env_names <- c("package:stats", "package:graphics", "package:grDevices",
                      "package:utils", "package:datasets", "package:methods"  ,
                      "Autoloads"    , "package:base")
  
  base_env_list <- sapply(base_env_names, function(x) mget(ls(x, all = TRUE), as.environment(x)))
  sum(lengths(base_env_list))
  
  sapply(base_env_list, lobstr::obj_size)
  round(sum(sapply(base_env_list, lobstr::obj_size)) / 1024^2, 2)
  ```

3. __<span style="color:red">Q</span>__: Predict the output of the following code:

    ```{r, eval = FALSE}
    x <- 1:1e6
    obj_size(x)
    
    y <- list(x, x)
    obj_size(y)
    obj_size(x, y)
    
    y[[1]][[1]] <- 10
    obj_size(y)
    obj_size(x, y)
    
    y[[2]][[1]] <- 10
    obj_size(y)
    obj_size(x, y)
    ```
    
   __<span style="color:green">A</span>__: Since `lobstr::obj_size()` currently throws returns very different values, we will use `unclass(pryr::obj_size())` for now.
   
   To predict the size of `x`, we first find out via `obj_size(integer(0))` that an integer takes 48 B. For every element of the integer vector additionally 4 B are needed and R allocates memory in chunks of 2, so 8 B at a time. This can be verified for example via `sapply(1:100, function(x) obj_size(integer(x)))`. Overall our prediction will result in 40 B + 1000000 * 4 B = 4000040 B:
    
    ```{r}
    x <- 1:1e6
    unclass(pryr::object_size(x))
    ```
    
   To predict the size of `y <- list(x, x)` consider that both list elements point to the same memory address. They share the same reference, which means that no additional memory is needed. A list takes 40 B in memory and 8 B for each element. Overall our prediction will result in x (4000040 B) + list of length 2 (40 B + 16 B):
    
    ```{r}
    y <- list(x, x)
    unclass(pryr::object_size(y))
    ```
    
   Since `x` and `y` are names with bindings to objects that point to the same reference, no additional memory is needed and our prediction is the maximum memory of both objects (y; 4000040 B):
    
    ```{r}
    unclass(pryr::object_size(x, y))
    ```
    
   The next one gets a bit more tricky. Since the first element of `y` becomes different to `x`, a completely new object is created in memory. Hence 10 is of type double (which triggers a silent coercion), the new object will take more memory. A double needs 40 B + length * 8 B (overall 8000040 B). So we get: first element of `y` (8000040 B) + second element of `y` (`x`; 4000040 B) + list of length 2 (40 B + 16 B) = 12000136 B as our prediction:
    
    ```{r}
    y[[1]][[1]] <- 10
    unclass(pryr::object_size(y))
    ```
    
   Again all elements of `x` are shared within `y` (`x` is the second element of `y`). So the overall memory usage corresponds to `y`'s:
    
    ```{r}
    unclass(pryr::object_size(x, y))
    ```
    
   In the next example also the second element of `y` gets the same value as the first one. However, R does not now, that it is the same as the first element, so a new object is created taking the same amount of memory:
    
    ```{r}
    y[[2]][[1]] <- 10
    unclass(pryr::object_size(y))
    ```
    
   Now `x` and `y` don't share any values anymore (from R's perspective) and their memory adds up:
    
    ```{r}
    unclass(pryr::object_size(x, y))
    ```

## Modify-in-place

1. __<span style="color:red">Q</span>__: Wrap the two methods for subtracting medians into two functions, then use the bench package to carefully compare their speeds. How does performance change as the number of columns increase?
    
   __<span style="color:green">A</span>__: First, let's define a function to create some random data and a function to subtract the median from each column.

    ```{r}
    create_random_df <- function(nrow, ncol) {
      random_matrix <- matrix(runif(nrow * ncol), nrow = nrow)
      as.data.frame(random_matrix)
    }
    
    subtract_medians <- function(x, medians){
      for (i in seq_along(medians)) {
        x[[i]] <- x[[i]] - medians[[i]]
      }
      x
    }
    ```

   We can then profile the performance, by benchmarking `subtact_medians()` on data frame- and list-input for a specified number columns. For list input the result has still to be coerced into a data frame, so that the output of both functions will be identical.

    ```{r}
    compare_speed <- function(ncol){
      df_input   <- create_random_df(nrow = 1e4, ncol = ncol)
      list_input <- as.list(df_input)
      medians <- vapply(df_input, median, numeric(1))
      
      bench::mark(`Data Frame` = subtract_medians(df_input,   medians),
                  List         = subtract_medians(list_input, medians) %>% 
                                   as.data.frame())
    }
    ```

   Then bench package allows us to run benchmark accros a grid of parameters easily. We will use it to slowly increase the number of columns of the random data.

    ```{r, warning=FALSE, message=FALSE}
    results <- bench::press(
      ncol = c(1, 5, 10, 50, 100, 200, 400, 600, 800, 1000, 1500),
      compare_speed(ncol)
    )
    ```

   The execution times for mean subtraction on data frames increase exponentially with the number of columns of the input data. This because, the data frame will be copied more often and the copy will also be bigger. For list-input the execution time increases linearly.
   
   For list input with less than ~ 800 columns, the cost of the additional data structure conversion will be relatively big. For very wide data frame input the overhead from the additional copies slows down the computation considerably. Apparently the choice of the faster function depends on the size of the data also.

    ```{r}
    library(ggplot2)
    ggplot(results, aes(ncol, median, col = expression)) +
      geom_point(size = 2) +
      geom_smooth() +
      labs(x = "Number of Columns of Input Data", y = "Computation Time",
           color = "Input Data Structure",
           title = "Benchmark: Median Subtraction")
    ```

2. __<span style="color:red">Q</span>__: What happens if you attempt to use `tracemem()` on an environment?

__<span style="color:green">A</span>__: `tracemem()` cannot be used to mark and trace environments.

    ```{r, error=TRUE}
    x <- new.env()
    tracemem(x)
    ```

   The error occurs because "it is not useful to trace NULL, environments, promises, weak references, or external pointer objects, as these are not duplicated" (see `?tracemem`). Environments are always modified in place.
